<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->
<configuration>
  <property>
    <name>http.agent.name</name>
    <value>ocean-oxygen</value>
  </property>
  <property>
    <name>http.agent.version</name>
    <value>1.13</value>
    <description>A version string to advertise in the User-Agent 
    header.
    </description>
  </property>
  <property>
    <name>http.agent.email</name>
    <value/>
    <description>An email address to advertise in the HTTP 'From' request
    header and User-Agent header. A good practice is to mangle this
    address (e.g. 'info at example dot com') to avoid spamming.
    </description>
  </property>
  <property>
    <name>http.agent.url</name>
    <value/>
    <description>A URL to advertise in the User-Agent header.  This will 
    appear in parenthesis after the agent name. Custom dictates that this
    should be a URL of a page explaining the purpose and behavior of this
    crawler.
    </description>
  </property>
  <property>
    <name>http.accept.language</name>
    <value>en-us,en-gb,en,de</value>
    <description>Value of the "Accept-Language" request header field.
    This allows selecting non-English language as default one to retrieve.
    It is a useful setting for search engines build for certain national group.
    </description>
  </property>
  <property>
    <name>db.max.outlinks.per.page</name>
    <value>200</value>
    <description>The maximum number of outlinks that we'll process for a page.
    If this value is nonnegative (&gt;=0), at most db.max.outlinks.per.page outlinks
    will be processed for a page; otherwise, all outlinks will be processed.
    </description>
  </property>
  <property>
    <name>db.url.filters</name>
    <value>true</value>
    <description>Filter urls when updating crawldb</description>
  </property>
  <property>
    <name>http.timeout</name>
    <value>5000</value>
    <description>The default network timeout, in milliseconds.</description>
  </property>
  <property>
    <name>plugin.includes</name>
    <value>protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor|more)|indexer-solr|scoring-similarity|parsefilter-naivebayes|urlnormalizer-(pass|regex|basic)|language-identifier|mimetype-filter</value>
  </property>
  <property>
    <name>fetcher.server.delay</name>
    <value>1.0</value>
    <description>The number of seconds the fetcher will delay between 
     successive requests to the same server. Note that this might get
     overridden by a Crawl-Delay from a robots.txt and is used ONLY if 
     fetcher.threads.per.queue is set to 1.
     </description>
  </property>
  <property>
    <name>http.content.limit</name>
    <value>1000000</value>
    <description>The length limit for downloaded content using the http://
    protocol, in bytes. If this value is nonnegative (&gt;=0), content longer
    than it will be truncated; otherwise, no truncation at all. Do not
    confuse this setting with the file.content.limit setting.
    </description>
  </property>
  <property>
    <name>fetcher.threads.fetch</name>
    <value>128</value>
    <description>The number of FetcherThreads the fetcher should use.
    This is also determines the maximum number of requests that are
    made at once (each FetcherThread handles one connection). The total
    number of threads running in distributed mode will be the number of
    fetcher threads * number of nodes as fetcher has one map task per node.
    </description>
  </property>
  <property>
    <name>http.redirect.max</name>
    <value>5</value>
    <description>The maximum number of redirects the fetcher will follow when
  trying to fetch a page. If set to negative or 0, fetcher won't immediately
  follow redirected URLs, instead it will record them for later fetching.
  </description>
  </property>
  <property>
    <name>db.injector.overwrite</name>
    <value>false</value>
    <description>Whether existing records in the CrawlDB will be overwritten
    by injected records.
    </description>
  </property>
  <property>
    <name>db.update.max.inlinks</name>
    <value>100</value>
    <description>Maximum number of inlinks to take into account when updating 
    a URL score in the crawlDB. Only the best scoring inlinks are kept. 
    </description>
  </property>
  <property>
    <name>generate.max.count</name>
    <value>1000</value>
    <description>The maximum number of urls in a single
    fetchlist.  -1 if unlimited. The urls are counted according
    to the value of the parameter generator.count.mode.
    </description>
  </property>
  <property>
    <name>fetcher.threads.per.queue</name>
    <value>2</value>
    <description>This number is the maximum number of threads that
      should be allowed to access a queue at one time. Setting it to 
      a value &gt; 1 will cause the Crawl-Delay value from robots.txt to
      be ignored and the value of fetcher.server.min.delay to be used
      as a delay between successive requests to the same server instead 
      of fetcher.server.delay.
     </description>
  </property>
  <property>
    <name>fetcher.server.min.delay</name>
    <value>0.0</value>
    <description>The minimum number of seconds the fetcher will delay between 
    successive requests to the same server. This value is applicable ONLY
    if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
    is turned off).</description>
  </property>
  <property>
    <name>fetcher.timelimit.mins</name>
    <value>45</value>
    <description>This is the number of minutes allocated to the fetching.
    Once this value is reached, any remaining entry from the input URL list is skipped 
    and all active queues are emptied. The default value of -1 deactivates the time limit.
    </description>
  </property>
  <property>
    <name>parser.timeout</name>
    <value>60</value>
    <description>Timeout in seconds for the parsing of a document, otherwise treats it as an exception and 
  moves on the the following documents. This parameter is applied to any Parser implementation. 
  Set to -1 to deactivate, bearing in mind that this could cause
  the parsing to crash because of a very long or corrupted document.
  </description>
  </property>
</configuration>

